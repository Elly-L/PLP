Q1. Define algorithmic bias and provide two examples.

Answer:
Algorithmic bias refers to systematic and unfair discrimination resulting from the data, design, or deployment of an AI system. It occurs when an algorithm produces outcomes that advantage or disadvantage groups based on characteristics such as gender, race, or age.

Examples:

Hiring algorithms that favor male candidates because the training data consisted mostly of previous male hires.

Credit scoring systems that assign lower loan approval scores to certain minority groups due to biased historical lending data.

Q2. Transparency vs Explainability

Answer:
Transparency refers to openness about how an AI system is built ‚Äî including its data sources, model architecture, training process, and intended use. It focuses on access to information.

Explainability refers to the ability of an AI system to provide understandable reasons for its individual predictions or actions.

Both are crucial because:

Transparency builds trust and accountability.

Explainability helps stakeholders understand, challenge, or verify decisions, especially in high-stakes domains like healthcare or law enforcement.

Q3. How GDPR impacts AI development in the EU

Answer:
GDPR directly shapes AI development by enforcing rules that protect user data, including:

Right to explanation, requiring clarity on automated decisions.

Data minimization, limiting the volume and type of personal data used for training.

User consent, requiring explicit permission for data processing.

Right to be forgotten, enabling deletion of personal data, which affects model retraining.

GDPR encourages responsible, privacy-focused AI design and restricts opaque, unregulated data use.

Ethical Principles Matching
Definition	Principle
Ensuring AI does not harm individuals or society.	B) Non-maleficence
Respecting users‚Äô right to control their data and decisions.	C) Autonomy
Designing AI to be environmentally friendly.	D) Sustainability
Fair distribution of AI benefits and risks.	A) Justice
‚úÖ Part 2 ‚Äî Case Study Analysis (40%)
Case 1: Biased Hiring Tool
1. Source of Bias

Training Data Bias: The model was trained on Amazon‚Äôs historical hiring data, which heavily favored male applicants.

Feature Selection Bias: Keywords or career paths linked stereotypically to male candidates were rewarded.

Sampling Imbalance: Underrepresentation of successful female candidates skewed the model's pattern recognition.

2. Fixes to Make the Tool Fairer

Rebuild the dataset using balanced and gender-neutral hiring examples.

Remove gender proxies (e.g., women‚Äôs colleges, maternity gaps, gendered wording).

Apply fairness constraints or debiasing algorithms during training, such as reweighing or adversarial debiasing.

3. Fairness Metrics to Evaluate

Disparate Impact Ratio (selection rate comparison)

Equal Opportunity Difference (true positive rate fairness)

Statistical Parity Difference

False Negative Rate Parity across gender groups

Case 2: Facial Recognition in Policing
Ethical Risks

Wrongful arrests due to misidentification of minorities.

Violation of privacy through mass surveillance.

Erosion of civil liberties and chilling effects on public freedom.

Reinforcement of systemic discrimination, especially towards Black and Brown communities.

Policies for Responsible Deployment

Mandatory third-party accuracy audits disaggregated by demographic groups.

Human-in-the-loop verification, disallowing autonomous arrests based solely on AI output.

Strict data governance, ensuring lawful collection and deletion policies.

Transparent reporting of error rates, deployment locations, and outcomes.

Prohibition in high-risk contexts where misidentification could cause harm (e.g., protests).

‚úÖ Part 3 ‚Äî Practical Audit (25%)

Below is a standard solution using AI Fairness 360.

A. Sample Code for COMPAS Audit
import pandas as pd
from aif360.datasets import CompasDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
import matplotlib.pyplot as plt

# Load COMPAS dataset
data = CompasDataset()

# Define privileged and unprivileged groups
priv = [{'race': 1}]       # Caucasian
unpriv = [{'race': 0}]     # African-American

# Compute fairness metrics
metric = BinaryLabelDatasetMetric(data, 
                                  privileged_groups=priv,
                                  unprivileged_groups=unpriv)

print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

# Visualizing false positive rate difference
# (requires a classifier; placeholder below)

B. 300-Word Report (Summary)

Report:
This audit examines racial bias in the COMPAS Recidivism Dataset using the AI Fairness 360 toolkit. The dataset has been widely critiqued for producing higher risk scores for African-American defendants compared to Caucasian defendants. Initial fairness metrics confirm the presence of bias: the Disparate Impact ratio falls significantly below the acceptable threshold of 0.8, indicating that unprivileged groups are disproportionately labeled as ‚Äúhigh risk.‚Äù The Statistical Parity Difference further demonstrates uneven positive classification rates between racial groups.

To understand outcome-level disparities, additional metrics such as False Positive Rate (FPR) and False Negative Rate (FNR) are evaluated. Prior studies and independent analyses reveal that African-American defendants are more likely to be incorrectly classified as high risk (false positives), while Caucasian defendants are more likely to be incorrectly classified as low risk (false negatives). This asymmetry creates serious ethical concerns, as people from unprivileged groups may face harsher judicial outcomes.

Mitigation strategies are then applied. Pre-processing methods such as Reweighing help rebalance the dataset by assigning fairness-driven weights to different subgroups. This reduces the influence of biased historical patterns. Alternative approaches such as adversarial debiasing or equalized odds post-processing can further harmonize error rates across groups.

In conclusion, the audit reveals clear racial disparities in the COMPAS system. The findings highlight the need for transparency, continuous monitoring, and algorithmic fairness constraints when deploying AI in criminal justice settings. No AI system should influence sentencing decisions without rigorous ethical safeguards.

‚úÖ Part 4 ‚Äî Ethical Reflection (5%)

Answer:
In my future AI projects, I will apply ethical principles by ensuring that data collection respects privacy and consent, and by documenting every dataset‚Äôs origin and limitations. I will evaluate models for bias using fairness metrics and include diverse test cases to avoid discrimination. Transparency will be prioritized by maintaining clear documentation and providing explanations for model outputs. Furthermore, I will incorporate human oversight in decision loops for high-stakes tasks. My goal is to design AI systems that are fair, accountable, and aligned with societal well-being.

üéØ Bonus Task ‚Äî Ethical AI in Healthcare (1-Page Policy Proposal)

Title: Ethical AI Use in Healthcare ‚Äî Policy Guidelines

1. Patient Consent Protocols

Patients must be clearly informed about what data is collected and how AI will use it.

Consent must be explicit, documented, and revocable.

Sensitive data (genetics, mental health, biometrics) requires enhanced safeguards.

2. Bias Mitigation Strategies

Perform demographic audits on all training datasets.

Ensure proportional representation during model development.

Apply fairness-aware algorithms and publish performance across demographic subgroups.

Continuous monitoring to detect drift-induced bias.

3. Transparency Requirements

Provide explainable AI outputs for clinical decisions (e.g., risk scores).

Maintain model cards describing data sources, limitations, and known risks.

Publish audit results and update logs for regulatory review.

Ensure clinicians understand the model‚Äôs assumptions and failure modes.

Conclusion
Healthcare AI must prioritize patient safety, fairness, and trust. These guidelines ensure responsible deployment aligned with ethical and legal standards.
